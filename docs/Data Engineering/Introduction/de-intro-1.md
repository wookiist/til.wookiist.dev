---
id: 'de-intro-1'
title: 'CH1 - 빅데이터의 기초 지식'
sidebar_position: 1
---
## Hadoop

다수의 컴퓨터에서 대량의 데이터를 처리하기 위한 시스템 

- 구글에서 개발한 분산 처리 프레임워크인 'MapReduce'를 참고하여 제작됨
- 기존에는 Hadoop에서 MapReduce를 실행하려면, 데이터 처리 내용을 Java로 개발해야 했음
- SQL 쿼리를 Hadoop에서 이용하기 위해 Hive가 개발됨 → 프로그래밍 없이 데이터를 집계 가능

## NoSQL

- 일반적으로 RDB보다 고속의 읽기/쓰기가 가능하고 분산 처리에 뛰어남
- 모여진 데이터를 나중에 집계하는 것이 아닌 애플리케이션으로 온라인으로 접속하는 DB

## Data Discovery

**대화형으로 데이터를 시각화하여 가치 있는 정보를 찾는 프로세스**

- BI 도구(Business Intelligence Tool)

## 빅데이터의 기술

**분산 시스템을 활용하면서 데이터를 순차적으로 가공해 나가는 일련의 구조**

## **데이터 파이프라인**

### 데이터 수집

데이터는 여러 장소에서 발생하고 각각 다른 형태를 보인다. 그리고 다양한 곳에서 서로 다른 기술로 데이터를 전송해온다. 데이터 전송(Data Transfer)에는 크게 두 가지 방식이 있다

![image](https://user-images.githubusercontent.com/16011260/134498088-68f9a027-ddff-44a3-989a-1949bd78c0cc.png)

- **벌크(bulk) 형**
    - 이미 어딘가에 존재하는 데이터를 정리해 추출하는 방법
    - DB와 파일 서버 등에서 정기적으로 데이터를 수집하는 데에 사용
- **스트리밍(streaming) 형**
    - 차례차례로 생성되는 데이터를 끊임없이 계속해서 보내는 방법
    - 모바일 애플리케이션과 임베디드 장비 등에서 널리 데이터를 수집하는 데에 사용

### 스트림 처리

- Stream Process
- 스트리밍 형 방법으로 받은 데이터는 실시간 처리가 우선시 됨

예를 들어, 과거 30분간 취합한 데이터를 집계하여 그래프를 만들려면, '시계열 데이터베이스'와 같은 실시간 처리를 지향하는 데이터베이스가 자주 이용된다. **스트림 처리의 결과를 시계을 데이터베이스에 저장함으로써, 지금 무슨 일이 일어나고 있는지 즉시 알 수 있다.**

- 다만, 장기적인 데이터 분석에는 적합하지 않음 → 장기적인 데이터 분석을 위해서는 보다 대량의 데이터를 저장하고 처리하는 데에 적합한 분산 시스템이 좋음
- 이 때는 어느 정도 정리된 데이터를 효율적으로 가공하기 위한 '**배치 처리(Batch Processing)**' 구조를 사용함

### 분산 스토리지

위의 절차를 거쳐 수집된 데이터는 **분산 스토리지(Distributed Storage)**에 저장된다. 분산 스토리지란 여러 컴퓨터와 디스크로부터 구성된 스토리지 시스템을 의미한다. 대표적으로 두 가지 선택이 있다.

- 오브젝트 스토리지
    - 한 덩어리로 모인 데이터에 이름을 부여해서 파일로 저장함
    - Amazon S3
- NoSQL DB
    - 애플리케이션에서 많은 데이터를 읽고 쓸 때 성능 면에서 우수함

### 분산 데이터 처리

분산 스토리지에 저장된 데이터를 처리하는 데는 '**분산 데이터 처리(Distributed Data Processing)**'  ****의 프레임워크가 필요하다. MapReduce가 사용된 부분이 바로 이 부분인데, **데이터 양과 처리의 내용에 따라 많은 컴퓨터 자원이 필요해진다.** 

분산 데이터 처리의 주 역할은 **나중에 분석하기 쉽도록 데이터를 가공해서 그 결과를 외부 데이터베이스에 저장하는 것**

빅데이터를 SQL로 집계할 때는 두 가지 방법이 있다.

- 분산 스토리지 상의 데이터를 SQl로 집계하기 위해 **쿼리 엔진**을 도입하는 것 → Hive는 그 한 가지 예로, 최근에는 '대화형 쿼리 엔진(Interactive ...)'도 개발됨
- 외부 데이터 웨어하우스 제품 이용 → 분산 스토리지에서 추출한 데이터를 데이터 웨어하우스에 적합한 형식으로 변환해야 함
    - 이 절차를 **ETL (Extract-Transform-Load)** 프로세스라고 함

### 워크플로 관리

전체 파이프라인의 동작을 관리하기 위해 사용한다. 매일 정해진 시간에 배치 처리를 스케줄대로 실행하고, 오류가 발생한 경우에는 관리자에게 통지하는 목적으로 사용한다.

데이터 파이프라인이 복잡해짐에 따라, 그것을 한 곳에서 제어하지 않았을 때의 어려움이 높아졌다.

## 데이터 웨어하우스와 데이터 마트

기존 방식대로 데이터 웨어하우스를 사용하는 프로세스로, 데이터 웨어하우스는 일반적인 RDB와는 달리 **대량의 데이터를 장기 보존하는**것에 최적화되어 있다. 따라서 **정리된 데이터를 한 번에 전송하는 것은 뛰어나지만, 소량의 데이터를 자주 쓰고 읽는 데는 적합하지 않다.** 

![image](https://user-images.githubusercontent.com/16011260/134676644-ba531105-6b2a-494c-b2de-2e258abf93b5.png)


데이터 소스에 보관된 Raw Data를 추출하고 필요에 따라 가공한 후 데이터 웨어하우스에 저장하기까지의 흐름이 ETL 프로세스다. 데이터 웨어하우스는 중요한 데이터 처리에 사용되기 때문에 아무때나 함부로 사용해 시스템 과부하를 초래해선 안 된다. 따라서 데이터 분석과 같은 목적으로 사용해야 하는 경우, 필요한 데이터만 추출하여 데이터 마트를 구축한다. 

## 데이터 레이크

빅데이터의 시대에는 ETL 프로세스 자체가 복잡해진다. 또한 모든 데이터가 데이터 웨어하우스를 가정하여 만들어지는 것도 아니다. 빅데이터의 시대에는 **우선 데이터가 있고, 나중에 테이블을 설계하는 것이 바람직하다.**

따라서 모든 데이터를 원래의 형태로 축적하고, 나중에 그것을 필요에 따라 가공하는 구조로 프로세스를 변경해야 한다. 여러 곳에서 데이터가 흘러들어 오는 '데이터를 축적하는 호수'에 비유해 데이터 축적 장소를 **데이터 레이크**라고 한다. 

![image](https://user-images.githubusercontent.com/16011260/134676697-feab5806-d053-4764-8829-273fe1e08866.png)

데이터 레이크는 단순한 스토리지이며, 이것만으론 데이터를 가공할 수 없다. 이를 위해 사용하는 것이 **MapReduce** 등의 분산 데이터 처리 기술이다. 

### 애드 혹 분석 및 대시보드 도구

시작 단계의 데이터 분석에선 자동화 등을 생각하지 않고 수작업으로 데이터를 집계할 수 있으면 된다. 이는 '일회성 데이터 분석'이라는 의미로 **애드 혹 분석(ad hoc analysis)**이라고 한다.

많은 경우에 애드 혹 분석에서는 데이터 마트를 만들지 않고, 데이터 레이크과 데이터 웨어하우스에 직접 연결하여 작업을 수행하는 경우가 많다. 

또한 수작업으로 데이터 분석뿐만 아니라 정기적으로 그래프와 보고서를 만들고 싶을 때도 있을 텐데, 이럴 때 많이 도입하는 것이 **대시보드 도구**이다. 

데이터 파이프라인의 큰 흐름은 변하지 않는다. 데이터를 모아서 축적하고 이를 통합하여 데이터 마트로 만들고 시각화 도구에서 접속하는 것이 큰 흐름이다. 중요한 것은 이러한 데이터의 흐름을 만드는 것이며, 그 과정에서 사용되는 기술은 교환할 수 있다. 

## 데이터를 수집하는 목적

'검색', '가공', '시각화'의 예

데이터를 모은 후에 무엇을 실시할지는 달성하고자 하는 목적에 따라 달라진다.

[이미지 TBD]

### 데이터 검색

대량의 데이터 중에서 조건에 맞는 것을 찾고 싶은 경우가 있다. 언제 무엇이 필요할지 모르기 때문에 시스템 로그, 고객의 행동 이력 등 발생하는 모든 데이터를 취득해 놓는다. 필요할 때 신속하게 검색할 수 있어야 하므로, 시스템에는 실시간 데이터 처리나 검색 엔진을 이용하여 키워드를 찾는 기능이 필요하다.

### 데이터 가공

업무 시스템의 일부로서 데이터 처리 결과를 이용하고 싶은 경우가 있다. 웹 사이트에서 추천 상품을 제안하거나 센서 데이터의 비정상적인 상태를 감지하여 통보하는 경우다. 목적이 명확하기 때문에 필요한 데이터를 계획적으로 모아서 데이터 파이프라인을 설계한다.

데이터 가공에는 자동화가 피수적이다. 따라서 워크플로 관리를 도입하여 반복적으로 테스팅을 수행해 시스템을 구축한다.

### 데이터 시각화

데이터를 시각적으로 봄으로써 알고 싶은 정보를 얻을 수 있다. 통계 분석 소프트웨어 또는 BI 도구 등으로 그래프를 만들고 거기에서 앞으로의 상황을 예측, 분석하여 의사 결정에 도움이 되도록 한다.